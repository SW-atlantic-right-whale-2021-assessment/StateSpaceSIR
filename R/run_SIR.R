
# R Script for Bayesian Assessment Model of Humpback Whales - developed from the code/equations
# in Zerbini et al. (2011)
# Adapted by Grant Adams and John Best for Zerbini et al. 2019

#' HUMPBACK SIR controls the sampling from the priors, the bisection and
#' likelihoods and the output functions
#'
#' @param file_name name of a file to identified the files exported by the
#'   function#'
#' @param allee_model Switch to determine the depensation model. 0 = no Allee effect; 1 = Hilborn et al 2014 P50 Allee Effect; 2 = Logistic Allee effect; 3 = Lin and Li 2002; 4 = Haider et al 2017. The depensation parameter is defined by \code{P50} using \link{make_prior_list}.
#' @param n_resamples number of resamples to compute the marginal posterior
#'   distributions
#' @param priors List of priors, usually generated using \link{make_prior_list}.
#'   Default is the default of \code{make_prior_list}. See details.
#' @param catch_multipliers List of catch multipliers, generated using \link{make_multiplier_list}
#'   Can either be estimated or explicitly provided. Default is \code{make_multiplier_list}.
#' @param target.Yr year of the target population estimate for the bisection
#'   method. Default is 2008
#' @param num.haplotypes number of haplotypes to compute minimum viable
#'   population (from Jackson et al., 2006 and IWC, 2007)
#' @param output.Yrs year for outputing the predicted abundance estimates.
#'   Default is 2008, but multiple years can be specified. For example, if
#'   outputs for 2005 and 2008 are needed, output.Yrs = c(2005, 2008)
#' @param abs.abundance R object containing year, estimate of absolute
#'   abundance, and CV (see example)
#' @param abs.abundance.key key to speficy if absolute abundance data are used
#'   in the likelihood. Default is TRUE
#' @param rel.abundance data.frame of relative abundance indices. Columns are Index, Year, IA.obs, and Variance-Covariance Matrix of Logs (column names do not matter for var-covar)
#' @param rel.abundance.key key to speficy if relative abundance data are used
#'   in the likelihood. Default is TRUE
#' @param count.data R object containing years, estimates of counts and effort.
#'   NOT USED
#' @param count.data.key key to speficy in count data are used. Default is
#'   FALSE. NOT USED
#' @param min_rmax Upper bound on rmax sampling (default to 0.118)
#' @param growth.rate.obs observed growth rate (1st element) and standard error
#'   (2nd element) as in Zerbni et al. (2011). If third element is FALSE, the
#'   growth rate is not included in the likelihood
#' @param growth.rate.Yrs Years for which the growth.rate.obs were computed (as
#'   in Zerbini et al., 2011)
#' @param catch.data R list where each layer contains a data.frame of the Year, Min Catch, and Max Catch of that catch time-series.
#' @param control A list of control parameters, usually generated by
#'   \code{sir_control}.
#' @param realized_prior Key to specify if realized prior is to be extracted. Default is FALSE.
#'
#' @return A \code{list} containing posterior samples and metadata
#'
#' Current default prior specification:
#' \code{
#' make_prior_list(r_max = make_prior(runif, 0, 0.118),
#'                 K = make_prior(use = FALSE),
#'                 var_N = make_prior(0),
#'                 N_obs = make_prior(runif, 500, 40000),
#'                 add_CV = make_prior(use = FALSE),
#'                 add_VAR_IA = make_prior(use = FALSE),
#'                 catch_sample = make_prior(runif, 0, 1),
#'                 z = make_prior(2.39),
#'                 Pmsy = make_prior(use = FALSE),
#'                 P50 = make_prior(0),
#'                 q_IA1 = make_prior(use = FALSE),
#'                 q_IA2 = make_prior(0),
#'                 q_count = make_prior(use = FALSE)
#'
#'  make_multiplier_list(c_mult_1 = make_prior(1))}
#'
#' @export
#'
#' @examples
#'
#' \dontrun{
#' StateSpaceSIR(file_name = "test.N2005",
#'              allee_model = 0,
#'              n_resamples = 100,
#'              priors = make_prior_list(),
#'              catch_multipliers = make_multiplier_list(),
#'              Klim = c(1, 500000),
#'              target.Yr = 2005,
#'              num.haplotypes = 0,
#'              tolerance.for.bisection = 0.0001,
#'              output.Yrs = c(2005, 2006),
#'              abs.abundance = Abs.Abundance.2005,
#'              abs.abundance.key = TRUE,
#'              rel.abundance = Rel.Abundance,
#'              rel.abundance.key = TRUE,
#'              count.data = Count.Data,
#'              count.data.key = FALSE,
#'              growth.rate.obs = c(0.074, 0.033, TRUE),
#'              growth.rate.Yrs = c(1995, 1996, 1997, 1998),
#'              catch.data = Catch.data,
#'              control = sir_control())
#' }
StateSpaceSIR <- function(file_name = "NULL",
                          allee_model = 0,
                          n_resamples = 1000,
                          priors = make_prior_list(),
                          catch_multipliers = make_multiplier_list(),
                          target.Yr = 2008,
                          num.haplotypes = 66,
                          output.Yrs = c(2008),
                          abs.abundance = Abs.Abundance,
                          abs.abundance.key = TRUE,
                          rel.abundance = Rel.Abundance,
                          rel.abundance.key = TRUE,
                          count.data = NULL,
                          count.data.key = FALSE,
                          min_rmax = 0.118,
                          growth.rate.obs = c(0.074, 0.033, TRUE),
                          growth.rate.Yrs = c(1995, 1996, 1997, 1998),
                          catch.data = Catch.data,
                          realized_prior = FALSE,
                          seed = 666,
                          control = sir_control()) {
    begin.time <- Sys.time()
    set.seed(666)

    ################################
    # Assigning variables
    ################################
    target.Yr <- target.Yr
    ## Use the first year of the projection is set as the first year in the
    ## catch series
    start_yr <- min(catch.data[[1]]$Year, na.rm = TRUE)
    for(i in 2:length(catch.data)){
        start_yr <- min(c(start_yr, catch.data[[i]]$Year), na.rm = TRUE)
    }

    ## The last year of the projection is set as the last year in the catch or
    ## abundance series, whichever is most recent
    end_yr <- max(max(abs.abundance$Year),
                  max(rel.abundance$Year),
                  output.Yrs)
    for(i in 1:length(catch.data)){
        end_yr <- max(c(end_yr, catch.data[[i]]$Year), na.rm = TRUE)
    }

    ## Setting the target year for the bisection method
    bisection.Yrs <- target.Yr-start_yr + 1
    ## Setting the years to project
    projection.Yrs <- end_yr-start_yr + 1
    Year <- seq(start_yr, end_yr, by = 1)

    # Expand the catch time series and fill missing values with 0
    catch_min <- list()
    catch_dif <- list()
    catch_max <- list()
    for(i in 1:length(catch.data)){
        catch.data[[i]] <- merge(data.frame(Year), catch.data[[i]], by="Year", all = TRUE)
        catch.data[[i]][is.na(catch.data[[i]])] <- 0

        # Get min and max of catch
        catch_original <- as.matrix(
            catch.data[[i]][,grep("catch", colnames(catch.data[[i]]), ignore.case = T)])
        catch_min[[i]] <- apply(catch_original, 1, FUN=min)
        catch_max[[i]] <- apply(catch_original, 1, FUN=max)
        catch_dif[[i]] <- catch_max[[i]] - catch_min[[i]]
    }

    ## Assigning the catch data
    n_catch_series <- length(catch.data)

    # Catch multiplier and parameters check
    if(length(catch_multipliers) != n_catch_series){
        stop("Number of catch multipliers (",
             length(catch_multipliers),
             ") does not equal number of catch periods (",
             n_catch_series,
             ")")
    }

    ## Determining the number of Indices of Abundance available
    num.IA <- max(rel.abundance$Index)

    ## Determining the number of Count Data sets available
    num.Count <- max(count.data$Index)

    ## Computing the value of sigma for the count data as in Zerbini et al. (2011)
    count.data$Sigma <- sqrt(log(1 + count.data$CV.IA.obs^2))

    ## Make var-covar into wide and tall with cov = 0 for different indices
    rel.var.covar.tall <-  subset(rel.abundance, select = -c(Index,Year,IA.obs))
    rel.var.covar.wide <- rel.var.covar.tall[which(rel.abundance$Index == 1),]
    rel.var.covar.wide <- rel.var.covar.wide[1:nrow(rel.var.covar.wide),1:nrow(rel.var.covar.wide)]

    rel.hess.tall <- solve(rel.var.covar.wide[1:nrow(rel.var.covar.wide), 1: nrow(rel.var.covar.wide)])

    if(num.IA>1){
        for(i in 2:length(unique(rel.abundance$Index))){
            var.cov.tmp <- as.matrix(rel.var.covar.tall[which(rel.abundance$Index == i),])
            var.cov.tmp <- var.cov.tmp[1:nrow(var.cov.tmp), 1:nrow(var.cov.tmp)]
            colnames(var.cov.tmp) <- NULL
            rownames(var.cov.tmp) <- NULL
            rel.var.covar.wide <- Matrix::bdiag(as.matrix(rel.var.covar.wide), var.cov.tmp)
            rel.hess.tall <- plyr::rbind.fill.matrix(rel.hess.tall, solve(var.cov.tmp))
        }
    }
    rel.var.covar.wide <- as.matrix(rel.var.covar.wide)
    rel.hess.wide <- solve(rel.var.covar.wide)

    ## Get relative-abundance year relative to each index (1:max_yr_index)
    group.center <- function(var,grp) {
        return(var-tapply(var,grp,min,na.rm=T)[grp])
    }
    rel.abundance$IndYear <- group.center(rel.abundance$Year, rel.abundance$Index)


    ## Computing the value of sigma as in Zerbini et al. 2011
    abs.abundance$Sigma <- sqrt(log(1 + abs.abundance$CV.obs^2))

    ## Computing the minimum viable population, if num.haplotypes=0, assumes no MVP
    MVP <- 3 * num.haplotypes


    ## Function to calculate Z if Pmsy is used
    NmsyKz <- function(z,Pmsy) { 1-(z+1)*Pmsy^z }
    if(priors$z$use & priors$Pmsy$use){
        warning("Priors were set on both Pmsy and Z, using the prior on Z")
    }

    ## Sample from prior for `z` or Pmsy (usually constant) if constant
    # - No depensation
    if(allee_model == 0){
        if (priors$z$use) {
            if(priors$z$class == "constant"){
                sample.z <- priors$z$rfn()
                sample.Pmsy <- uniroot(NmsyKz,z=sample.z,lower=0,upper=1)$root
            }
        } else {
            if(priors$Pmsy$class == "constant"){
                sample.Pmsy <- priors$Pmsy$rfn()
                sample.z <- uniroot(NmsyKz,Pmsy=sample.Pmsy,lower=1,upper=100)$root
            }
        }
    }

    ## Start the loop
    i <- 0
    ## Keep track of number of draws
    draw <- 1
    Cumulative.Likelihood <- 0

    #Creating output vectors
    #-------------------------------------
    sir_names <- c("r_max", "K", "var_N", "z", "Pmsy", "P50", paste0("catch_multiplier_", 1:length(catch_multipliers)) , "catch_parameter",
                   "sample.N.obs", "add_CV", "add_VAR_IA","Nmin", "YearMin",
                   "violate_MVP", paste0("N", target.Yr), paste0("N", output.Yrs),
                   paste0("ROI_IA", unique(rel.abundance$Index)),
                   paste0("q_IA1", unique(rel.abundance$Index)),
                   paste0("q_IA2", unique(rel.abundance$Index)),
                   paste0("ROI_Count", unique(count.data$Index)),
                   paste0("q_Count", unique(count.data$Index)),
                   "NLL.IAs", "NLL.Count", "NLL.N", "NLL.GR", "NLL", "Likelihood",
                   "Max_Dep",paste0("status", target.Yr), paste("status", output.Yrs, sep = ""), "draw", "save")

    proc_error_save <- matrix(NA, nrow = n_resamples, ncol = projection.Yrs-1)
    resamples_output <- matrix(NA, nrow = n_resamples, ncol = length(sir_names))
    resamples_trajectories <- matrix(NA, nrow = n_resamples, ncol = projection.Yrs)
    catch_trajectories <- matrix(NA, nrow = n_resamples, ncol = projection.Yrs)
    colnames(catch_trajectories) =  paste0("Catch_", Year)

    if (control$progress_bar) {
        pb <- txtProgressBar(min = 0, max = n_resamples, style = 3)
    }

    #Initiating the SIR loop
    while (i < n_resamples) {
        #Sampling from Priors
        #-------------------------------
        save <- FALSE #variable to indicate whether a specific draw is kept

        # Sampling for catch_multiplier and high-low sample
        sample.catch_multipliers <- sapply(catch_multipliers, function(x) x$rfn())
        sample.catch_parameter <- priors$catch_sample$rfn()

        catches <- rep(0, length(Year))

        for(p in 1:length(catch.data)){
            catches <- catches + (catch_min[[p]] + catch_dif[[p]] * sample.catch_parameter) * sample.catch_multipliers[p]
        }

        #Sampling for r_max
        sample.r_max <- priors$r_max$rfn()
        while (sample.r_max > min_rmax) {
            sample.r_max <- priors$r_max$rfn()
        }

        ## Sampling from the N.obs prior
        sample.N.obs <- priors$N_obs$rfn()

        ## Prior on additional CV
        if (priors$add_CV$use) {
            sample.add_CV <- priors$add_CV$rfn()
        } else {
            sample.add_CV <- 0
        }

        if (priors$add_VAR_IA$use) {
            sample.add_VAR_IA <- sqrt(priors$add_VAR_IA$rfn())
        } else {
            sample.add_VAR_IA <- 0
        }

        ## Sample from prior for variance of process error
        sample.var_N <- priors$var_N$rfn()
        sample.proc.error <- rlnorm(projection.Yrs-1, meanlog = 0, sdlog = sqrt(sample.var_N)) # Random process error

        ## Sample depensation parameter
        sample.P50 = priors$P50$rfn()

        ## Sample from prior for `z` or Pmsy (usually constant) if random
        # - Depensation set by allee_model:
        # -- 0 = no Allee effect; 1 = Hilborn et al 2014 P50 Allee Effect; 2 = Logistic Allee effect; 3 = Lin and Li 2002; 4 = Haider et al 2017.
        if(allee_model == 0){
            if (priors$z$use) {
                if(priors$z$class == "function"){
                    sample.z <- priors$z$rfn()
                    sample.Pmsy <- uniroot(NmsyKz,z=sample.z,lower=0,upper=1)$root
                }
            } else {
                if(priors$Pmsy$class == "function"){
                    sample.Pmsy <- priors$Pmsy$rfn()
                    sample.z <- uniroot(NmsyKz,Pmsy=sample.Pmsy,lower=1,upper=100)$root
                }
            }
        }

        if(allee_model == 1){ # Hilborn et al 2014
            if (priors$z$use) {
                sample.z <- priors$z$rfn()
                sample.Pmsy <- uniroot(pmsy_z_hilborn,z=sample.z, k = 100, r = sample.r_max, q = sample.P50, lower=0, upper=1)$root
            } else {
                sample.Pmsy <- priors$Pmsy$rfn()
                sample.z <- uniroot(pmsy_z_hilborn,Pmsy=sample.Pmsy, k = 100, r = sample.r_max, q = sample.P50, lower= 0.000001,upper=100)$root

            }
        }

        if(allee_model == 2){ # Logistic
            if (priors$z$use) {
                sample.z <- priors$z$rfn()
                sample.Pmsy <- uniroot(pmsy_z_logistic,z=sample.z, k = 100, r = sample.r_max, q = sample.P50, lower=0, upper=1)$root
            } else {
                sample.Pmsy <- priors$Pmsy$rfn()
                sample.z <- uniroot(pmsy_z_logistic,Pmsy=sample.Pmsy, k = 100, r = sample.r_max, q = sample.P50, lower= 0.000001,upper=100)$root

            }
        }

        if(allee_model == 3){ # Lin and Li 2002
            if (priors$z$use) {
                sample.z <- priors$z$rfn()
                sample.Pmsy <- uniroot(pmsy_z_linli,z=sample.z, k = 100, r = sample.r_max, q = sample.P50, lower=0.4, upper=1)$root
            } else {
                # Re-sample Pmsy until reasonable solution, given depensation parameter
                sample.z <- try(uniroot(pmsy_z_linli, Pmsy = .5, q = .2, r = .2, k - 100, lower= .4,upper=1)$root, silent = TRUE)
                while (class(sample.z) == "try-error") {
                    sample.Pmsy <- priors$Pmsy$rfn()
                    sample.z <- try(uniroot(pmsy_z_linli,Pmsy=sample.Pmsy, k = 100, r = sample.r_max, q = sample.P50, lower= 0.000001,upper=100)$root, silent = TRUE)
                }
            }
        }

        if(allee_model == 4){ # Haider et al 2017
            if (priors$z$use) {
                sample.z <- priors$z$rfn()
                sample.Pmsy <- uniroot(pmsy_z_haider,z=sample.z, k = 100, r = sample.r_max, q = sample.P50, lower=0.4, upper=1)$root
            } else {
                # Re-sample Pmsy until reasonable solution, given depensation parameter
                sample.z <- try(uniroot(pmsy_z_haider, Pmsy = .5, q = .2, r = .2, k - 100, lower= .4,upper=1)$root, silent = TRUE)
                while (class(sample.z) == "try-error") {
                    sample.Pmsy <- priors$Pmsy$rfn()
                    sample.z <- try(uniroot(pmsy_z_haider,Pmsy=sample.Pmsy, k = 100, r = sample.r_max, q = sample.P50, lower= 0.000001,upper=100)$root, silent = TRUE)
                }

            }
        }

        ## Sampling from q priors if q.prior is TRUE; priors on q for indices of
        ## abundance
        q.error = FALSE
        if (priors$q_IA1$use) {
            q.sample.IA1 <- replicate(num.IA, priors$q_IA1$rfn())
            q.sample.IA2 <- replicate(num.IA, priors$q_IA2$rfn())

            # if(sum(q_vec <= 0) > 0){
            #     q.error = TRUE
            # }
        } else {
            ## FIXME: -9999 is probably not a good sentinel value here; NA?
            q.sample.IA1 <- rep(-9999, num.IA)
            q.sample.IA2 <- rep(-9999, num.IA)
            # q_vec <- -9999 * exp(rel.abundance$IndYear * 0)
        }

        ##priors on q for count data
        if (priors$q_count$use) {
            q.sample.Count <- replicate(num.Count, priors$q_count$rfn())
        } else {
            ## FIXME: Sentinel -9999 again
            q.sample.Count <- rep(-9999, length(unique(count.data$Index)))
        }

        ## Conduct logistic bisection
        sample.K <-  try(LOGISTIC.BISECTION.K(K.low = control$K_bisect_lim[1],
                                              K.high = 1e8,
                                              allee_model,
                                              r_max = sample.r_max,
                                              z = sample.z,
                                              P50 = sample.P50,
                                              num_Yrs = bisection.Yrs,
                                              start_yr = start_yr,
                                              target.Pop = sample.N.obs,
                                              catches = catches,
                                              proc_error = sample.proc.error,
                                              MVP = MVP,
                                              tol = control$K_bisect_tol),
                         silent = TRUE)

        ## If population is too variable because of process error, give error and set likelihood to 0
        if(class(sample.K) == "try-error"){
            sample.K = 999
            K.error = TRUE
        } else{
            sample.K = sample.K
            K.error = FALSE
        }

        #Computing the predicted abundances with the samples from the priors
        #----------------------------------------
        Pred_N <- GENERALIZED_LOGISTIC(allee_model = allee_model,
                                       r_max = sample.r_max,
                                       K = sample.K,
                                       N1 = sample.K,
                                       z = sample.z,
                                       P50 = sample.P50,
                                       start_yr = start_yr,
                                       num_Yrs = projection.Yrs,
                                       catches = catches,
                                       proc_error = sample.proc.error,
                                       MVP = MVP)


        #Computing the predicted ROI for the IAs and Count data, if applicable
        #----------------------------------------
        #For IAs
        if (rel.abundance.key & !q.error) {
            Pred.ROI.IA <- COMPUTING.ROI(data = rel.abundance,
                                         Pred_N = Pred_N$Pred_N,
                                         start_yr = start_yr)
        } else {
            Pred.ROI.IA <- rep(0, num.IA)
        }

        #For Count Data
        if (count.data.key) {
            Pred.ROI.Count <- COMPUTING.ROI(data = count.data,
                                            Pred_N = Pred_N$Pred_N,
                                            start_yr = start_yr)
        } else {
            Pred.ROI.Count <- rep(0, num.Count)
        }

        #Calculate Analytical Qs if rel.abundance.key is TRUE
        #---------------------------------------------------------
        if (rel.abundance.key) {
            if (!priors$q_IA1$use) {
                q.sample.IA2 <- replicate(num.IA, priors$q_IA2$rfn())
                q.sample.IA1 <- CALC.ANALYTIC.Q.MVLNORM(rel.abundance,
                                                        rel.var.covar.tall,
                                                        rel.hess.tall,
                                                        Pred_N$Pred_N,
                                                        start_yr,
                                                        sample.add_VAR_IA,
                                                        beta = q.sample.IA2,
                                                        num.IA)
            } else {
                q.sample.IA1 <- q.sample.IA1
                q.sample.IA2 <- q.sample.IA2
                # q_vec = q_vec
            }
        }


        ## Calculate Analytical Qs if count.data.key is TRUE
        ## (NOT USED YET - AZerbini, Feb 2013)
        if (count.data.key) {
            if (!priors$q_count$use) {
                q.sample.Count <- CALC.ANALYTIC.Q(count.data,
                                                  Pred_N$Pred_N,
                                                  start_yr,
                                                  sample.add_CV,
                                                  num.Count)
            } else {
                q.sample.Count <- q.sample.Count
            }
        }

        if (control$verbose > 3) {
            message("r_max = ", sample.r_max,
                    " N.obs = ", sample.N.obs,
                    " K = ", sample.K,
                    " Pred_N.target = ", Pred_N$Pred_N[bisection.Yrs],
                    " q.IAs = ", q.sample.IA1,
                    " q.Count = ", q.sample.Count)
        }

        #Compute the likelihoods
        #--------------------------------
        # (1) relative indices (if rel.abundance.key is TRUE)
        if (rel.abundance.key & !q.error) {
            lnlike.IAs <- LNLIKE.MVLNORM.IAs(rel.abundance,
                                             rel.var.covar.wide,
                                             Pred_N$Pred_N,
                                             start_yr,
                                             q.sample.IA1,
                                             q.sample.IA2,
                                             sample.add_VAR_IA,
                                             TRUE)
        } else {
            lnlike.IAs <- 0
        }

        # (2) count data (if count.data.key is TRUE)
        if (count.data.key) {
            lnlike.Count <- LNLIKE.IAs(count.data,
                                       Pred_N$Pred_N,
                                       start_yr,
                                       q.sample.Count,
                                       sample.add_CV,
                                       log=TRUE)
        } else {
            lnlike.Count <- 0
        }

        # (3) absolute abundance
        if (abs.abundance.key) {
            lnlike.Ns <- LNLIKE.Ns(abs.abundance,
                                   Pred_N$Pred_N,
                                   start_yr,
                                   sample.add_CV,
                                   log=TRUE)
        } else {
            lnlike.Ns <- 0
        }

        # (4) growth rate if applicable
        if (growth.rate.obs[3]) {
            Pred.GR <- PRED.GROWTH.RATE(growth.rate.Yrs=growth.rate.Yrs,
                                        Pred_N=Pred_N$Pred_N,
                                        start_yr=start_yr)
            lnlike.GR <- LNLIKE.GR(Obs.GR=growth.rate.obs[1],
                                   Pred.GR=Pred.GR,
                                   GR.SD.Obs=growth.rate.obs[2])
        } else {
            lnlike.GR <- 0
        }

        if (control$verbose > 2) {
            message("lnlike.IAs = ", lnlike.IAs,
                    " lnlike.Count = ", lnlike.Count,
                    " lnlike.Ns = ", lnlike.Ns,
                    " lnlike.GR = ", lnlike.GR)
        }

        ## These use the likelihoods in Zerbini et al. (2011)
        NLL <- lnlike.IAs[[1]] + lnlike.Count[[1]] + lnlike.Ns[[1]] + lnlike.GR[[1]]
        Likelihood <- exp(-NLL)
        if (control$verbose > 1) {
            message("NLL = ", NLL,
                    " Likelihood = ", Likelihood)
        }


        ## If population fell below minimum viable population size, set likelihood to 0
        if (Pred_N$Violate_Min_Viable_Pop) {
            Likelihood <- 0
            if (control$verbose > 0) {
                message("MVP violated on draw", draw)
            }
        }

        ## If population was too variable because of process error, set likelihood to 0
        if (K.error) {
            Likelihood <- 0
            if (control$verbose > 0) {
                message("Population dynamics too variable on draw", draw)
            }
        }

        ## If q <= 0
        if (q.error) {
            Likelihood <- 0
            if (control$verbose > 0) {
                message("Q less than 0 on draw", draw)
            }
        }

        Cumulative.Likelihood <- Cumulative.Likelihood + Likelihood

        # Trick to just extract realized prior
        if(realized_prior){
            Cumulative.Likelihood <- 2 * control$threshold
        }

        if (!Pred_N$Violate_Min_Viable_Pop) {

            while (Cumulative.Likelihood > control$threshold & i < n_resamples) {
                if (control$verbose > 0) {
                    message("sample = ", i, " draw = ", draw)
                }
                if (control$verbose > 1) {
                    message("draw = ", draw,
                            " Likelihood = ", Likelihood,
                            " Cumulative = ", Cumulative.Likelihood)
                }
                save <- TRUE
                Cumulative.Likelihood <- Cumulative.Likelihood-control$threshold
                resamples_trajectories[i+1,] <- Pred_N$Pred_N
                catch_trajectories[i+1,] <- catches
                proc_error_save[i+1,] <- sample.proc.error
                resamples_output[i+1,] <- c(sample.r_max,
                                            sample.K,
                                            sample.var_N,
                                            sample.z,
                                            sample.Pmsy,
                                            sample.P50,
                                            sample.catch_multipliers,
                                            sample.catch_parameter,
                                            sample.N.obs,
                                            sample.add_CV,
                                            sample.add_VAR_IA,
                                            Pred_N$Min_Pop,
                                            ifelse(length(Pred_N$Min_Yr) == 1, Pred_N$Min_Yr, "Multiple"),
                                            Pred_N$Violate_Min_Viable_Pop,
                                            c(Pred_N$Pred_N[target.Yr - start_yr + 1]),
                                            c(Pred_N$Pred_N[output.Yrs - start_yr + 1]),
                                            Pred.ROI.IA,
                                            q.sample.IA1,
                                            q.sample.IA2,
                                            Pred.ROI.Count,
                                            q.sample.Count,
                                            lnlike.IAs[[1]],
                                            lnlike.Count[[1]],
                                            lnlike.Ns[[1]],
                                            lnlike.GR[[1]],
                                            NLL,
                                            Likelihood,
                                            Pred_N$Min_Pop / sample.K,
                                            c(Pred_N$Pred_N[target.Yr - start_yr + 1] /
                                                  sample.K),
                                            c(Pred_N$Pred_N[output.Yrs - start_yr + 1] /
                                                  sample.K),
                                            draw,
                                            save)
                i <- i+1
                if (control$progress_bar) {
                    setTxtProgressBar(pb, i)
                }
            }
        }
        draw <- draw+1
    }

    # Save outputs
    resamples_output <- data.frame(resamples_output)
    resamples_output[] <- lapply(resamples_output, function(x) as.numeric(as.character(x)))
    names(resamples_output) <- sir_names
    if(!is.null(file_name)){
        write.csv(resamples_output,
                  paste0(file_name, "_", "resamples_output.csv"))
    }

    resamples_trajectories <- data.frame(resamples_trajectories)
    resamples_trajectories[] <- lapply(resamples_trajectories, function(x) as.numeric(as.character(x)))
    names(resamples_trajectories) <- paste0("N_", Year)
    if(!is.null(file_name)){
        write.csv(resamples_trajectories,
                  paste0(file_name, "_", "resamples_trajectories.csv"))
    }

    catch_trajectories <- data.frame(catch_trajectories)
    catch_trajectories[] <- lapply(catch_trajectories, function(x) as.numeric(as.character(x)))
    names(catch_trajectories) <- paste0("Catch_", Year)
    if(!is.null(file_name)){
        write.csv(catch_trajectories,
                  paste0(file_name, "_", "catch_trajectories.csv"))
    }

    proc_error_save <- data.frame(proc_error_save)
    proc_error_save[] <- lapply(proc_error_save, function(x) as.numeric(as.character(x)))
    names(proc_error_save) <- paste0("Proc_error_", Year[1:(projection.Yrs-1)])
    if(!is.null(file_name)){
        write.csv(proc_error_save,
                  paste0(file_name, "_", "proc_error.csv"))
    }

    resamples.per.samples <- draw / n_resamples
    if(resamples.per.samples < 3){
        warning("Number of resamples per sample is ",
                round(resamples.per.samples, 1),
                ", use higher threshold value.")
    } else if (resamples.per.samples > 20) {
        warning("Number of resamples per sample is ",
                round(resamples.per.samples, 1),
                ", use lower threshold value.")
    }

    end.time <- Sys.time()
    if (control$verbose > 0) {
        message("Time to Compute = ", (end.time-begin.time))
    }

    return_list <- list(call = call,
                        file_name = file_name,
                        Date.Time = Sys.time(),
                        Time.to.compute.in.minutes = paste((end.time-begin.time) / 60),
                        threshold = control$threshold,
                        Ratio.Resamples.per.Sample = paste("1 resample",
                                                           ":",
                                                           resamples.per.samples,
                                                           "samples"),
                        resamples_output = resamples_output,
                        resamples_trajectories = resamples_trajectories,
                        catch_trajectories = catch_trajectories,
                        inputs = list(allee_model = allee_model,
                                      draws = draw,
                                      n_resamples = n_resamples,
                                      prior_r_max = priors$r_max,
                                      catch_multipliers = catch_multipliers,
                                      priors_N_obs = priors$N_obs,
                                      target.Yr = target.Yr,
                                      start_yr = start_yr,
                                      MVP = paste("num.haplotypes = ",
                                                  num.haplotypes,
                                                  "MVP = ",
                                                  3 * num.haplotypes),
                                      tolerance = control$K_bisect_tol,
                                      output.Years = output.Yrs,
                                      abs.abundance = abs.abundance,
                                      catch.data = catch.data,
                                      realized_prior = realized_prior))
    if(rel.abundance.key){ return_list$inputs$rel.abundance = rel.abundance}
    if(count.data.key){ return_list$inputs$count.data = count.data}

    class(return_list) <- "SIR" # Defines class for object

    return(return_list)
}
